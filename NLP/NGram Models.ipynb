{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data: Wikipedia\n",
    "WE will use the <a href=\"https://pypi.org/project/wikipedia/\">wikipedia python api</a> to download some text data. This api allows you to download wikipedia pages and create your text corpus programmatically.\n",
    "**Study the API docs. You can also do this lab in a different language than English.**\n",
    "\n",
    "The procedure we will use:\n",
    "1. Download texts in two different categories. For example, if one of your categories is 'political figures' then one of your searches could be 'Hilary Clinton' and then we can download all the texts in wikipedia that contain the terms 'Hilary Clinton' in the title. Try to have around ten possible names in each category.\n",
    "2. Split the texts into sentences and each sentence into tokens, representing it as a list (using the nltk tools).\n",
    "3. It will be convenient to add a STARTSENT and ENDSENT marker to each one.\n",
    "\n",
    "**YOu need to complete the code below (preferably by generalising the for loop to populate both corpora)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "\n",
    "#Feel free to change these, but choose potential titles that belong to the same category\n",
    "#List of topics or names in category 1, e.g. rock musicians\n",
    "cat1 = ['Jimi Hendrix', 'Janis Joplin', 'Steve Winwood', 'Siouxsie Sioux'] \n",
    "cat2 = [] #List of wiki articles in category 2 -- enter your own\n",
    "\n",
    "corp1 = [] #This will be a list of sentences in texts in cat1, each represented as a list\n",
    "corp2 = [] #Do the same for this one in cat 2\n",
    "\n",
    "for c in cat1:    \n",
    "    page_hits = wikipedia.search(c) #all the pages with this search term in the title\n",
    "    #print(page_hits) #Uncomment if you want to see the hits\n",
    "    \n",
    "    for hit in page_hits:\n",
    "        page = wikipedia.page(hit) #Download the actual page\n",
    "        text = page.content #the actual text\n",
    "        \n",
    "        #tokenise and add to corpus - NB This is a list of lists\n",
    "        for s in sent_tokenize(text): \n",
    "            corp1.append(['STARTSENT'] + word_tokenize(s) + ['ENDSENT'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Train, dev and test\n",
    "We're going to need training and test data, but also **dev** data, which we'll use below for determining the best smoothing parameter.\n",
    "\n",
    "Complete the function below which, given your dataset and proportions `tr`, `dv` and `ts` (for train, dev and test respectively -- e.g. 0.8, 0.1, and 0.1) returns a random split of your data into three different sets. (*Hint*: Look at the similar function we used in our Naive Bayes lab. But remember this time the data is just a python list). Create separate train/dev/test splits for both your corpora.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def split_train_test(data, tr, dv, ts):\n",
    "    '''Splits data into train, dev and test sets based on given proportions.'''\n",
    "    \n",
    "    shuffled_indices = np.random.permutation(len(data)) #randomise indices\n",
    "    tr_size = int(len(data) * tr)\n",
    "    dv_size = int(len(data) * dv)\n",
    "    ts_size = int(len(data) * ts)\n",
    "    \n",
    "    #indices of items in train, test and dev\n",
    "    train_idx = shuffled_indices[:tr_size] #training set indices\n",
    "    dev_idx = shuffled_indices[tr_size: tr_size+dv_size]\n",
    "    test_idx = shuffled_indices[tr_size+dv_size:]\n",
    "    \n",
    "    train_set = [data[i] for i in train_idx]\n",
    "    dev_set = [data[i] for i in dev_idx]\n",
    "    test_set = [data[i] for i in test_idx]\n",
    "    return train_set, dev_set, test_set\n",
    "\n",
    "corp1_train, corp1_dev, corp1_test = split_train_test(corp1, 0.8, 0.1, 0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Models\n",
    "\n",
    "We'll use some biult-in nltk functions to build our models. NLTK provides the functions `bigram`, `trigram` to extract n-grams of sizes 2 and 3. In addition, we'll use a python `defaultdict` to hold our ngram models. This is available in python's `collections` library. Though basically a python dictionary, it provides some high-performance functionality.\n",
    "\n",
    "The strategy we'll use is exemplified below:\n",
    "\n",
    "1. Extract n-grams of different sizes and build a python default dict mapping histories to the final word in the ngram, to frequencies. (e.g. in a trigram model, we have (word1, word2): count)\n",
    "2. Convert the frequencies to probabilities using add-k smoothing, for some variable quantity `k`\n",
    "\n",
    "An example is done for you below, for bigrams. **You need to generalise this function to create n-grams of variable lengths (from 1 to 3).** \n",
    "\n",
    "The additional function `to_probabilities` should work for any order of ngram. **You also need to complete this function.** Note that this function takes a parameter `k`, which is 1 by default. The idea is to use `k=1` for add-one smoothing, or indeed any value of `k` (if 0, it's just a Maximum Likelihood Estimate).\n",
    " \n",
    "Some things to consider:\n",
    "* To make the function generalisable, we can represent keys in our dictionary as tuples of variable length. For an ngram of length n, the dictionary will have the form `model[(history)][wn] = count`. For example, the trigram *I like sugar* would be represented as `model[('I', 'like')]['sugar'] = count` while the bigram *I like* would be `model[('I')]['like'] = count`.\n",
    "* The example below does not lowercase words. This means that 'Jimi' and 'jimi' are not the same word. Consider whether you want this or not.\n",
    "* For a given set of ngrams starting with some history `h`, probabilities should sum to approximately 1. Check that this is the case for a few examples. (*Hint*: in a bigram model, `model[(h)].values()` gives you all the frequencies for any bigram starting with history `h`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "#Builds bigrams - change this to build ngrams of various lengths up to 3\n",
    "def build_bigrams(data):\n",
    "    #the incantation below creates a dictionary of dictionaries\n",
    "    #{word: {word: frequency}}\n",
    "    #for trigrams w1,w2,w3, we can do: model[(w1,w2)][w3] so keys are tuples of length 2\n",
    "    model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    \n",
    "    for s in data:                \n",
    "        for w1, w2 in bigrams(s):           \n",
    "            model[tuple(w1)][w2] += 1 \n",
    "    return model\n",
    "\n",
    "#Convert counts to probabilities\n",
    "# Add k for smoothing (default k = 1)\n",
    "def to_probabilities(model, k=1):  \n",
    "    vocab_size = len(model)\n",
    "    prob_model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "                  \n",
    "    #transform to probabilities\n",
    "    # with smoothing parameter\n",
    "    for history in model:\n",
    "        total_count = float(sum(model[history].values()))\n",
    "        \n",
    "        for word in model[history]:\n",
    "            prob_model[history][word] = (model[history][word] + k) / (total_count+ (vocab_size*k))\n",
    "    \n",
    "    return prob_model\n",
    "    \n",
    "#Build model with frequencies\n",
    "c1_bigram_freq = build_bigrams(corp1_train)\n",
    "\n",
    "#Convert our models to (smoothed) probabilities\n",
    "c1_bigram_prob = to_probabilities(c1_bigram_freq)          \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Working with unseen values\n",
    "\n",
    "Our model won't contain all ngrams. If we use a smoothing parameter `k`, we can also use `k` to estimate the probability of an unseen case, given the vocabulary size. The function below needs to be completed to achieve this. \n",
    "\n",
    "Remember that it is safe to do this because we have discounted all our actual probabilities by an amount proportional to `k`. If we don't do this, unseens are just assigned probability 0.\n",
    "\n",
    "**Complete the function below to return the probability of a new, previously unseen ngram (assuming we pass the function the ngram in the form of a tuple)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute probability of unseen bigrams (w1,w2) with add-k\n",
    "#NB: Pass the model with counts, not probabilities!\n",
    "def unseen_prob(freq_model, k, ngram_tuple):\n",
    "    if k == 0:\n",
    "        return 0\n",
    "    \n",
    "    voc_size = len(freq_model)\n",
    "    \n",
    "    #unigram case \n",
    "    if len(ngram_tuple) == 1:\n",
    "        total_freq = sum(freq_model.values())\n",
    "        return k / (total_freq + voc_size)\n",
    "\n",
    "    #Ngrams for n > 1\n",
    "    else:\n",
    "        history = ngram_tuple[:-1]\n",
    "        hist_freq = sum(freq_model[history].values())\n",
    "        return k / (hist_freq + voc_size)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perplexity\n",
    "\n",
    "Given a model, we evaluate by computing its perplexity on a test set.\n",
    "\n",
    "Complete the function below (your code should go where indicated). It should take an ngram model (both frequencies and probabilities) and a set of ngrams (we can use our function above to get them) extracted from a test set or dev set, and return the perplexity value.\n",
    "\n",
    "Things to consider:\n",
    "1. If you used some value of `k` for smoothing your training ngram model, you need that `k` for perplexity. This is because, if there's some ngram in the test set which happens not to be in training (and there will be), you use `k` as your estimate. If you left `k` equal to 0, the perplexity calculation must simply ignore any ngrams in dev/test that are not in train. This is undesirable (so do train your model with some value of `k`).\n",
    "2. YOu don't need to have probabilities for the test or dev set, just the ngrams themselves (we'll use our function above to obtain them).\n",
    "3. Don't multiply probabilities as you're likely to get overflow errors. Add logs instead. WE typically use logs to base 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'build_bigrams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8e1e027ffe8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;31m#If we used k =/= 0 to train our model, we need to supply this to our perplexity estimate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;31m# This is used for any ngrams which are not found in the train set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mc1_bigrams_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_bigrams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorp1_dev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0mdev_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorp1_dev\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mpp_dev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperplexity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc1_bigram_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc1_bigrams_dev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'build_bigrams' is not defined"
     ]
    }
   ],
   "source": [
    "#Perplexity function: given a trained model and a list of ngrams from dev/test\n",
    "#Also needs smoothing parameter k if used, else zero by default\n",
    "#test_n is the number of tokens in the test corpus\n",
    "def perplexity(prob_model, freq_model, test_ngrams, test_n, k=1):    \n",
    "    probabilities = [] #accumulate individual probs here\n",
    "    \n",
    "    #Iterate through the test ngram set\n",
    "    # Compute the perplexity by extracting the probability for a TEST ngram\n",
    "    # using the TRAINED_MODEL\n",
    "    for history in test_ngrams:\n",
    "        if history in model:\n",
    "            for last_word in test_ngrams[history]:\n",
    "                if last_word in model[history]:\n",
    "                    probabilities.append(np.log2(model[history][last_word]))\n",
    "\n",
    "                elif k > 0: #useless to include for k=0\n",
    "                    unseen_p = unseen_prob(freq_model, k, history + tuple(last_word))\n",
    "                    probabilities.append(np.log2(unseen_p))\n",
    "        elif k > 0:\n",
    "            unseen_p = unseen_prob(freq_model, k, history + tuple(last_word))\n",
    "            probabilities.append(np.log2(unseen_p))\n",
    "    \n",
    "    #take the sum of log probs and multiple by -1/N\n",
    "    prob_sum = np.sum(probabilities) * -(1/test_n)\n",
    "    #take exponent to \n",
    "    perplexity = np.power( 2, prob_sum )\n",
    "    return perplexity\n",
    "        \n",
    "#Example: get the bigrams for the corp1 dev set and compute perplexity on that\n",
    "#If we used k =/= 0 to train our model, we need to supply this to our perplexity estimate\n",
    "# This is used for any ngrams which are not found in the train set\n",
    "c1_bigrams_dev = build_bigrams(corp1_dev)\n",
    "dev_size = np.sum([len(s) for s in corp1_dev])\n",
    "pp_dev = perplexity(c1_bigram_prob, c1_bigram_freq, c1_bigrams_dev, dev_size, k=1)\n",
    "print(pp_dev)\n",
    "\n",
    "#Same for test set\n",
    "c1_bigrams_test = build_bigrams(corp1_test)\n",
    "test_size = np.sum([len(s) for s in corp1_test])\n",
    "pp_test = perplexity(c1_bigram_prob, c1_bigram_freq, c1_bigrams_test, test_size, k=1)\n",
    "print(pp_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Optimising k\n",
    "\n",
    "At this point, you have a function to build language models, and a function to compute their perplexity. Now, let's use perplexity to find the best amount `k` to add during `add-k` smoothing.\n",
    "\n",
    "Do the following:\n",
    "\n",
    "1. Train different langauge models (bigram or trigram) with values of k ranging from `0.1` to `1` at increments of `0.1`. **NB** To train a trigram model, you'll need to reconsider the function `build_bigrams` above. How can you change it to make it build trigram models?\n",
    "2. For each model, compute its perplexity **on the dev set**\n",
    "3. Choose the model with the lowest perplexity and compute its final perplexity on the test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 The effect of data\n",
    "\n",
    "Having established your optimal value for smoothing, try computing the perplexity of a model trained on your first corpus (corp1), on the test set of a second corpus (corp2).\n",
    "\n",
    "You would typically observe an increase in perplexity here: your model will do better on a test set sampled from the same data, compared to data related to a different topic.\n",
    "\n",
    "Consider why this could be. What does this tell you about how models capturre aspects of content and style, from one type of text (or topic) to another?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Random generation\n",
    "\n",
    "Given an n-gram model, where start/end symbols have also been included (as they were above), we can write a random sentence generator. With a bigram model, for example, this would proceed as follows:\n",
    "\n",
    "1. Choose an ngram that begins with `STARTSENT`, `(STARTSENT, x)` with some probability proportional to the model probability. Add `x` to your sentence.\n",
    "2. Until the `ENDSENT` symbol is reached: take the last word of the previously sampled ngram `(_, x)`, and find another ngram `(x, y)` that begins with `x`. Concatenate `y` to the sentence.\n",
    "3. Stop if you sample an ngram whose last word is `ENDSENT`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lin3011",
   "language": "python",
   "name": "lin3011"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
